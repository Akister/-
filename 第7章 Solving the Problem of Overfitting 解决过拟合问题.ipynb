{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overfitting  过拟合\n",
    "如果特征过多，可能因缺乏足够多的数据去限制算法给出好的假设，从而出现学习得到的预测函数可以很好的匹配训练集，但是无法泛化应用于新的样本。\n",
    "\n",
    "解决过拟合的思路：\n",
    "\n",
    "- 1.减少参与训练的特征数量\n",
    "    - 手动选择参与训练的特征；\n",
    "    - 模型选择算法；  \n",
    "- 2.Regularization\n",
    "    - 保留所有特征参与训练，但减少参数$\\theta_j$的大小\n",
    "    - 数据具有很多特征时依然效果良好，每个特征都参与样本预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization 正则化\n",
    "由于参数$\\theta_0,\\theta_1,\\cdots,\\theta_n$越小，分类算法将得到更平滑的假设函数，即假设函数更简单，从而更不容易出现过拟合(Overfitting)\n",
    "\n",
    "经过Regularization的$Cost\\ Function$为：$$J(\\theta)=\\frac{1}{2m}\\left[\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2\\right]$$\n",
    "其中$\\lambda$称为Regularization参数，用来调节：训练假设函数匹配数据的拟合度和保持参数$\\theta$够小以避免过拟合两个目标之间的关系。\n",
    "\n",
    "然后继续求$\\min \\limits_{\\theta} J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Regularization 线性回归正则化\n",
    "经过Regularization的$Cost\\ Function$为：$$J(\\theta)=\\frac{1}{2m}\\left[\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2\\right]$$\n",
    "\n",
    "#### **梯度下降法**\n",
    "\n",
    "过程变为：$$\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "\n",
    "$$\\begin{eqnarray*}\\theta_j&:=&\\theta_j-\\alpha\\left[\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{\\lambda}{m}\\theta_j\\right]\\\\\n",
    "&:=&\\theta_j(1-\\alpha\\frac{\\lambda}{m})-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\end{eqnarray*}$$\n",
    "其中$j=1,2,3,\\cdots,n$\n",
    "\n",
    "$\\theta_0,\\theta_1,\\theta_2,\\cdots,\\theta_n$在一次迭代中需同时更新\n",
    "\n",
    "#### 法方程\n",
    "样本矩阵表示为：$$X=\\left[\\begin{array}{ccc}(x^{(1)})^T\\\\\\vdots\\\\(x^{(m)})^T\\end{array}\\right]\\ ,\\ y=\\left[\\begin{array}{ccc}(y^{(1)})\\\\\\vdots\\\\(y^{(m)})\\end{array}\\right]$$\n",
    "其中$X$为$m*(n+1)$阶矩阵，$y$为$m*1$阶矩阵\n",
    "\n",
    "要求$\\min \\limits_{\\theta}\\  J(\\theta)$，则使：\n",
    "$$\\theta=(X^TX+\\lambda\\left[\\begin{array}{ccc}0&0&0&\\cdots&0\\\\0&1&0&\\cdots&0\\\\0&0&1&\\cdots&0\\\\\\vdots&\\vdots&\\vdots&\\cdots&\\vdots\\\\0&0&0&\\vdots&1\\end{array}\\right])^{-1}X^Ty$$\n",
    "其中$\\lambda>0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression Regularization 逻辑回归正则化\n",
    "经过Regularization的逻辑回归代价函数为：\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m\\left[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\\right]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_{j}^2$$\n",
    "其中$\\theta_0$不参与正则化；\n",
    "\n",
    "则梯度下降法变为：\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha\\left[\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{\\lambda}{m}\\theta_j\\right]$$\n",
    "其中$j=1,2,\\cdots,n$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 未能理解Regularization \n",
    "\n",
    "1.Adding many new features to the model makes it more likely to overfit the training set.\n",
    "\n",
    "2.Adding a new feature to the model always results in equal or better performance on the examples not in the training set.\n",
    "\n",
    "3.Introducing regularization to the model always results in equal or better performance on the training set.\n",
    "\n",
    "4.Introducing regularization to the model always results in equal or better performance on examples not in the training set.\n",
    "\n",
    "5.Because regularization outputs values $0\\le h_{\\theta}(x)\\le 1$,it's range of output value can only be \"shrunk\" slightly by regularization anyway,so regularization is generally not helpful for it.$\\quad$ ** (wrong) ** \n",
    "\n",
    "6.Using too large a value of $\\lambda$ can cause you hypothesis to underfit the data.$\\quad$ **(right)**\n",
    "\n",
    "7.Because regularization causes $J(\\theta)$ to no longer be convex,gradient descent may not always converge to the global mininum(when $\\lambda>0$,and when using an appropriate learning rate $\\alpha$).$\\quad$ ** (wrong)**\n",
    "\n",
    "8.Using a very large of $\\lambda$ cannot hurt the performance of hypothesis;the only reason we do not set $\\lambda$ to be too large is to avoid numerical problems.$\\quad$ **(wrong) **\n",
    "\n",
    "9.Adding many new features to the model helps prevent overfitting on the training set.\n",
    "\n",
    "10.Consider a classification problem,Adding regularization may cause your classifier to incorrectly classify some training examples(which it had correctly classified when not using regularization,i.e,when $\\lambda>0$).\n",
    "\n",
    "11.Using too large a value of $\\lambda$ can cause your hypothesis to overfit the data;this can be avoided by reducing $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features 多特征\n",
    "\n",
    "**符号定义**\n",
    "\n",
    "$m$: 训练集中样本数量\n",
    "\n",
    "$n$：特征数量\n",
    "\n",
    "$x^{(i)}$：训练集中第$i$个样本的特征向量$\\quad i\\in(1,m)$\n",
    "\n",
    "$x_{j}^{(i)}$：训练集中第$i$个样本的特征向量的第$j$个值$\\quad i\\in(1,m),j\\in(1,n)$\n",
    "\n",
    "**假设函数**\n",
    "\n",
    "$h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_nx_n$\n",
    "\n",
    "为简化起见，为每一个样本的特征向量添加一个值1，即定义$x_{0}$=1,则样本的特征向量为：$x=\\left(\\begin{array}{c} x_0 \\\\x_1\\\\\\vdots\\\\x_n \\end{array}\\right)$，则$\\theta=\\left(\\begin{array}{c}\\theta_0\\\\\\theta_1\\\\\\vdots\\\\\\theta_n\\end{array}\\right)$\n",
    "\n",
    "则假设函数$h_{\\theta}(x)$变化为：$$\\begin{eqnarray*}h_{\\theta}(x)&=&\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_nx_n$ \\\\\n",
    "&=&\\theta^Tx \\quad\\mbox{($\\theta^T$为$\\theta$的转置)}\\end{eqnarray*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多变量梯度下降法\n",
    "\n",
    "假设函数：$h_{\\theta}(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+\\cdots+\\theta_nx_n$\n",
    "\n",
    "代价函数：$$J(\\theta_0,\\theta_1,\\cdots,\\theta_n)=\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "梯度下降法为重复下列过程：$$\\begin{eqnarray*}\\theta_j&:=&\\theta_j-\\alpha\\frac{\\partial J(\\theta_0,\\theta_1,\\cdots,\\theta_n)}{\\partial \\theta_j}\\\\&=&\\theta_j-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_{{j}}^{(i)}\\end{eqnarray*}$$ $j\\in (0,n),\\mbox{$\\theta_j$同时进行更新}$\n",
    "\n",
    "具体展开为：\n",
    "\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_{{0}}^{(i)}$$\n",
    "$$\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_{{1}}^{(i)}$$\n",
    "$$\\vdots$$\n",
    "$$\\theta_n:=\\theta_n-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_{{n}}^{(i)}$$\n",
    "\n",
    "线性回归常用技巧：特征值缩放，将所有特征值的变化范围变换为相近的变换距离，有利于加快梯度下降法收敛。通常进行正则化，$$x_i=\\frac{x_i-average(x)}{maximum(x)-minimum(x)},\\mbox {这样$x_i \\in (-1,1)$}$$\n",
    "\n",
    "为确保梯度下降法随着迭代次数增加趋向收敛，实时画出迭代次数与代价函数的图形来判断。可能出现的情形为：\n",
    "\n",
    "**1.**若学习速率$\\alpha$太小，则代价函数收敛太慢；\n",
    "\n",
    "**2.**若学习速率$\\alpha$太大，则代价函数可能不会随着迭代次数增加而减小，或者不收敛进而发散。\n",
    "通常取$\\alpha$为$\\cdots$,0.001，0.01，0.1，1,$\\cdots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数分析 法向量方程\n",
    "针对代价函数：$$J(\\theta_0,\\theta_1,\\cdots,\\theta_n)=\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "当有$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}=\\cdots=0\\ (for\\ every\\ j)$$\n",
    "\n",
    "即可求得$J(\\theta)$的最小值。\n",
    "\n",
    "针对$m$个训练样本$(x^{(1)},y^{(1)}),\\cdots,(x^{(n)},y^{(n)})$，每个样本具有$n$个特征，构造特征矩阵：\n",
    "\n",
    "首先对每个样本的特征向量在前面添加一个值1，即：$$x_{(i)}=\\left( \\begin{array}{c}x_0^{(i)}\\\\x_1^{(i)}\\\\\\vdots\\\\x_n^{(i)}\\end{array} \\right)$$\n",
    "其中$x_0^{(i)}=1$；\n",
    "\n",
    "接下来构造矩阵：$$\\begin{eqnarray*}X&=&\\left( \\begin{array}{c}(x^{(1)})^T\\\\(x^{(2)})^T\\\\\\vdots\\\\(x^{(m)})^T\\end{array}\\right)\\\\\n",
    "&=&\\left( \\begin{array}{c}1&x_1^{(1)}&\\cdots&x_n^{(1)}\\\\1&x_1^{(2)}&\\cdots&x_n^{(2)}\\\\\\vdots&\\vdots&\\cdots&\\vdots\\\\1&x_1^{(m)}&\\cdots&x_n^{(m)}\\end{array}\\right)\\end{eqnarray*}$$\n",
    "其中$X$为$m×(n+1)$矩阵；\n",
    "$$y=\\left(\\begin{array}{c}y^{(1)}\\\\y^{(2)}\\\\\\vdots\\\\y^{(m)}\\end{array}\\right)$$\n",
    "则:$$\\theta=(X^TX)^{-1}X^Ty$$\n",
    "其中$\\theta=(\\theta_0,\\theta_1,\\cdots,\\theta_n)$.\n",
    "\n",
    "\n",
    "优点：无需选择学习速率$\\alpha$，无需进行迭代\n",
    "\n",
    "缺点：需要计算矩阵的逆，当样本特征数目过大时，运算量太大，效果不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法与法向量方程法比较\n",
    "\n",
    "\n",
    "**梯度下降法**                  |      ** 法向量方程法 **     \n",
    "---------------------             | ---------------------------------\n",
    "1.需要选择学习速率$\\alpha$          |  1.无需选择学习速率$\\alpha$\n",
    "2.需要进行多次迭代               |  2.无需进行迭代\n",
    "3.当样本特征数目很大时，依然效果很好   |  3.需要计算特征矩阵的逆，当样本特征数目过大（$>10^6$）时，计算量过大，效果不佳\n",
    "                           |  4.需建立在特征矩阵可逆的情形下才有效\n",
    "法向量方程法矩阵不可逆情况：\n",
    "\n",
    "1.特征冗余，即存在特征之间线性依赖的情况；\n",
    "\n",
    "2.特征数目过多，而训练样本过少\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function 代价函数\n",
    "针对输入样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$\n",
    "\n",
    "定义：\n",
    "\n",
    "$L$：神经网络总的层数\n",
    "\n",
    "$s_l$：神经网络第$l$层中Logistic Unit的个数（未包含需添加的偏移单元$a_0^{i}$）\n",
    "\n",
    "假设样本输入为一个多分类问题($K$个类别)，则Cost Function为：\n",
    "$$J(\\theta)=-\\frac{1}{m}\\left[\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}log(h_{\\theta}(x))_k+(1-y_k^{(i)})log(1-(h_{\\theta}(x))_k)\\right]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{yj}^{(l)})^2$$\n",
    "其中$h_{\\theta}(x)$是一个$K\\mbox{×}1$的输出矩阵，$(h_{\\theta}(x))_i$为第$i$个输出值，$\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{yj}^{(l)})^2$不包含每层需添加的$\\theta_0^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation:Backpropagation algorithm 梯度计算：反向传播算法\n",
    "定义：\n",
    "\n",
    "$\\delta_j^{(l)}$：神经网络中第$l$层第$j$个节点的误差；\n",
    "\n",
    "假设一个4层的神经网络，对于第4层中的每一输出Unit有：\n",
    "\n",
    "$\\delta_j^{(4)}=a_j^{(4)}-y_j$\n",
    "\n",
    "则有：\n",
    "\n",
    "$\\delta^{(3)}=(\\theta^{(3)})^T\\delta^{(4)}.*g^{'}(z^{(3)})$$\\quad$ 经计算有$g^{'}(z^{(3)})=a^{(3)}.*(1-a^{(3)})$\n",
    "\n",
    "$\\delta^{(2)}=(\\theta^{(2)})^T\\delta^{(3)}.*g^{'}(z^{(2)})$$\\quad$ 经计算有$g^{'}(z^{(2)})=a^{(2)}.*(1-a^{(2)})$\n",
    "\n",
    "第一层为样本特征输入层不存在误差。\n",
    "\n",
    "### 反向传播算法描述\n",
    "针对输入样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})}$\n",
    "\n",
    "设$\\Delta_{ij}^{l}=0(\\mbox{for all $i,j,l$})$;\n",
    "\n",
    "$i$从$1$到$m$\n",
    "\n",
    "$\\quad$ 令$a^{(1)}=x^{(i)}$\n",
    "\n",
    "$\\quad$ 先使用前向传播算法计算$a^{(l)}\\quad (\\mbox{$for\\ all\\ l=2,3,\\cdots,L$})$\n",
    "\n",
    "$\\quad$ 使用$y^{(i)}$计算$\\delta^{(L)}=a^{(L)}-y^{(i)}$\n",
    "\n",
    "$\\quad$ 依次计算$\\delta^{(L-1)}，\\delta^{(L-2)}，\\cdots，\\delta^{(2)}$\n",
    "\n",
    "$\\quad$ 则$\\Delta_{ij}^{(l)}:=\\Delta_{ij}^{(l)}+a_j^{(l)}\\delta_i^{(l+1)}$\n",
    "\n",
    "$\\quad$ 最终有$D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda\\theta_{ij}^{(l)}\\ \\mbox{$if\\ j \\neq0$}$\n",
    "\n",
    "$\\quad$ $\\quad$$\\quad$ $D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}\\quad\\quad\\quad \\mbox{$if\\ j=0$}$\n",
    "\n",
    "$\\quad$ 则：$$\\frac{\\partial J(\\theta)}{\\partial \\theta_{ij}^{(l)}}=D_{ij}^{(l)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrolling parameter 参数展开\n",
    "将多层神经网络的参数重组进一个矩阵里以方便算法函数传参\n",
    "\n",
    "以4层的神经网络为例：\n",
    "\n",
    "设每层的参数矩阵依次为：$\\theta_1,\\ \\theta_2,\\ \\theta_3$\n",
    "\n",
    "每层的梯度矩阵依次为：$D1,\\ D2,\\ D3$\n",
    "\n",
    "则将上述矩阵重组为：\n",
    "$$thetaVec=[\\theta_1(:);\\theta_2(:);\\theta_3(:)]$$\n",
    "$$DVec=[D1(:);D2(:);D3(:)]$$\n",
    "在算法函数内部计算时使用reshape()函数进行拆分以分层计算\n",
    "\n",
    "### 算法过程描述\n",
    "1.首先初始化参数矩阵$\\theta_1,\\ \\theta_2,\\ \\theta_3$\n",
    "\n",
    "2.将上述参数矩阵展开为$initialTheta$以便传进函数$fminunc(@costFunction,initialTheta,options)$\n",
    "\n",
    "3.其中$function [jval,gradientVec]=costFunction(thetaVec)$内部过程为：\n",
    " \n",
    "$\\quad$ 3.1 从$thetaVec$中进行重组得到$\\theta_1,\\ \\theta_2,\\ \\theta_3$\n",
    "\n",
    "$\\quad$ 3.2 使用前向传播和反向传播算法计算$D1,\\ D2,\\ D3$和$J(\\theta)$,然后将$D1,\\ D2,\\ D3$展开为$gradientVec$传出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking 梯度检查\n",
    "为检查上述算法编写正确，以计算近似偏导数与算法计算得到$DVec$相比是否相差巨大来验证\n",
    "\n",
    "### Gradient CheCking 梯度检查算法描述\n",
    "$for\\ i\\ =1:n$\n",
    "\n",
    "$\\quad thetaPLus=theta;$\n",
    "\n",
    "$\\quad thetaPLus(i)=thetaPlus(i)+EPSILON;$\n",
    "\n",
    "$\\quad thetaMinus=theta;$\n",
    "\n",
    "$\\quad thetaMinus=thetaMinus(i)-EPSILON;$\n",
    "\n",
    "$\\quad gradApprox(i)=((J(thetaPlus)-J(thetaMinus))/(2*EPSILON);$\n",
    "\n",
    "$\\quad end$\n",
    "\n",
    "其中$gradApprox$与$DVec$维度相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Initialozation 随机初始化\n",
    "如果$\\theta$以全0进行初始化，将会导致隐藏层中对应相同输入的Unit的值相同，需要打破这种对称性(Symmetry Breaking)\n",
    "\n",
    "即将$\\theta$进行随机初始化：\n",
    "\n",
    "初始化$\\theta$为每一个值都介于$[-\\epsilon,\\epsilon] \\quad (i.e. \\ -\\epsilon\\geq\\theta_{ij}^{(l)}\\geq\\epsilon)$\n",
    "\n",
    "$e.g$ $$theta1=rand(10,11)*(2*INIT\\_EPSILON)-INIT\\_EPSILON$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Neural Network 训练神经网络的流程\n",
    "#### 1.Pick a network architecture 选择神经网络结构（神经网络隐藏层数，每一隐藏层中Units的个数）\n",
    "$\\quad$ 至少有1个隐藏层，当隐藏层的数量大于1个时，尽量每一个隐藏层的Units数量相同（通常情况下Units的数量越多越好，隐藏层中Units的数量要大于输入特征数量）\n",
    "#### 2.Randomly initialize parameter 随机初始化参数矩阵$\\theta$，$\\theta$通常会被初始化为接近0的很小的值\n",
    "#### 3.Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "#### 4.Implement code to compute cost function $J(\\theta)$\n",
    "#### 5.Implement backpropagation to compute partial derivatives $\\frac{\\partial J(\\theta)}{\\partial \\theta_{jk}^{(l)}}$\n",
    "#### 6.Use gradient checking to compare $\\frac{\\partial J(\\theta)}{\\partial \\theta_{jk}^{(l)}}$ computed using backpropagation vs. using numerical of gradient of $J(\\theta)$.Then disable gradient checking code.\n",
    "#### 7.Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\\theta)$ as a function of parameters $\\theta$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification 分类器\n",
    "对数据集的不同预测结果进行数学表示：\n",
    "\n",
    "$y=0 \\mbox{表示预测结果为“恶性、欺诈、$\\cdots$”}$;\n",
    "\n",
    "$y=1 \\mbox{表示预测结果为“良性、非欺诈、$\\cdots$”}$;\n",
    "\n",
    "#### 分类器算法目标\n",
    "\n",
    "通过训练集训练得到预测函数$0\\le h_{\\theta}(x)\\le1$，设定预测函数$h_{\\theta}(x)$输出阈值为0.5，即：\n",
    "\n",
    "1.若$h_{\\theta}(x)\\geq 0.5$,则预测标记$\"y=1\"$\n",
    "\n",
    "2.若$h_{\\theta}(x)< 0.5$,则预测标记$\"y=0\"$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression 逻辑回归（分类算法）\n",
    "Sigmoid函数（也叫Logistic函数）：$$g(z)=\\frac{1}{1+e^{-z}}$$,其中$z$为实数；\n",
    "\n",
    "逻辑回归模型为：$$\\begin{eqnarray*}h_{\\theta}(x)&=&g(\\theta^Tx)\\\\&=&\\frac{1}{1+e^{-\\theta^Tx}}\\end{eqnarray*}$$\n",
    "\n",
    "其中对于任意的$x\\in R,\\mbox{有}0\\le h_{\\theta}(x)\\le1$;\n",
    "\n",
    "模型解释：预测函数$h_{\\theta}(x)$的结果表示对于每一个输入样例$x$给出样例$x$样本标记为$y=1$的估计概率：\n",
    "\n",
    "数学语言表示：$h_{\\theta}(x)=P(y=1|x;\\theta)$,\n",
    "\n",
    "即给定参数$\\theta$，样本$x$的样本标记$y=1$的概率。\n",
    "\n",
    "#### Decision Boundary （决策边界）\n",
    "由于Sigmoid函数$g(z)=\\frac{1}{1+e^{-z}}$具有以下特性：\n",
    "\n",
    "1.当$z\\ge0$时，有$g(z)\\ge0.5$\n",
    "\n",
    "2.当$z<0$时，有$g(z)<0.5$\n",
    "\n",
    "令$z=\\theta^Tx$代入逻辑回归预测函数$h_{\\theta}(x)$，即有：\n",
    "\n",
    "1.当$\\theta^Tx\\ge0$时，有预测函数$h_{\\theta}(x)\\ge0.5$，则可以预测样例$x$的样本标记为$y=1$;\n",
    "\n",
    "2.当$\\theta^Tx<0$时，有预测函数$h_{\\theta}(x)<0.5$，则可以预测样例$x$的样本标记为$y=0$;\n",
    "\n",
    "即$\\theta^Tx=0$成为预测函数$h_{\\theta}(x)$的决策边界，在边界不同位置的样本分属不同的类别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function （代价函数）\n",
    "Logistic Regression 代价函数：$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)},y^{(i)}))$$\n",
    "其中$$Cost(h_{\\theta}(x),y)=\\begin{equation}\\begin{cases}-log(h_{\\theta}(x))\\quad if,y=1\\\\-log(1-h_{\\theta}(x))\\quad if, y=0\\end{cases}\\end{equation}$$\n",
    "又$Cost(h_{\\theta}(x),y)$可以简化为：$$Cost(h_{\\theta}(x),y)=-ylog(h_{\\theta}(x))-(1-y)log(1-log(h_{\\theta}(x)))$$\n",
    "#### 最终\n",
    "$$\\begin{eqnarray*}J(\\theta)&=&\\frac{1}{m}\\sum_{i=1}^mCost(h_{\\theta}(x^{(i)}),y^{(i)})\\\\\n",
    "&=&-\\frac{1}{m}[\\sum_{i=1}^my^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression （逻辑回归梯度下降法求参数$\\theta$）\n",
    "由逻辑回归的代价函数为：\n",
    "$$J(\\theta)=-\\frac{1}{m}[\\sum_{i=1}^m(y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$$\n",
    "要想求得$min_{\\theta}\\ J(\\theta)$:\n",
    "$$\\theta_{j}:=\\theta_{j}-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$$\n",
    "其中$h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta^Tx}}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他最优化方法求参数$\\theta$\n",
    "首先定义代价函数：\n",
    "$$function [jval\\ ,gradient]=costFunction(theta)\\\\\n",
    "jval=(theta(1)-5)^2+\\cdots+(theta(2)-5)^2;\\\\\n",
    "gradient=zeros[2\\ ,1];\\\\\n",
    "gradient(1)=2*(theta(1)-5);\\\\\n",
    "gradient(2)=2*(theta(2)-5);$$\n",
    "\n",
    "然后通过调用命令：$$options=optimset('GradObj'\\ ,'on'\\ ,'MaxIter'\\ ,'100');\\\\\n",
    "initialTheta=zeros[2\\ ,1];\\\\\n",
    "[optTheta\\ ,functionVal\\ ,exitFlag]=fminunc(@costFunction\\ ,initialTheta\\ ,options);$$\n",
    "\n",
    "代价函数通用格式为：\n",
    "$$function [jval\\ ,gradient]=costFunction(theta)\\\\\n",
    "jval=[code\\ to\\ compute\\ J(\\theta)];\\\\\n",
    "gradient=zeros[n\\ ,1];\\\\\n",
    "gradient(1)=[code\\ to\\ compute\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}];\\\\\n",
    "\\vdots\\\\\n",
    "gradient(n)=[code\\ to\\ compute\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}}];$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multiclass Classification （多类别分类器）\n",
    "样本具有多个标签（类别）（$\\ge3$），如天气：多云，雨天，晴天，大雪，$\\cdots$\n",
    "\n",
    "**One vs all (One vs rest)**\n",
    "\n",
    "轮流对训练样本每个标签训练一个分类器：当训练一个标签时，数据集中具有此标签的数据做为一个类别，数据集剩余数据整体作为一个类别，进行训练分类器\n",
    "\n",
    "具体描述：对数据集中的每一个类别$i$训练出一个分类器$h_{\\theta}^{(i)}(x)$来预测样本$x$的类别$y=i$的概率\n",
    "\n",
    "当有一个新的样本$x$输入，则选择所有分类器中$\\max \\limits_{i}\\ h_{\\theta}^{(i)}(x)$来确定样本的$x$的类别"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

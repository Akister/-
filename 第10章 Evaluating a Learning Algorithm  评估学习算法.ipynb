{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Learning Algorithm 调试学习算法\n",
    "\n",
    "** 常用的调试学习算法的方法 **\n",
    "\n",
    "1.获取更多的训练样本(Fixed High Variance)\n",
    "\n",
    "2.缩小参与训练的特征数量(Fixed High Variance)\n",
    "\n",
    "3.添加额外的特征(Fixed High Bias)\n",
    "\n",
    "4.添加多项式特征(Fixed High Bias)\n",
    "\n",
    "5.加大正则化系数$\\lambda$(Fixed High Bias)\n",
    "\n",
    "6.减小正则化系数$\\lambda$(Fixed High Variance)\n",
    "\n",
    "## Machine Learning Diagnostic 机器学习算法诊断\n",
    "\n",
    "** 测试机器学习算法是否在有效的运行，并且进行改善机器学习算法的性能 **\n",
    "\n",
    "## Evaluating Hypothesis 评估假设函数\n",
    "\n",
    "首先将原始训练集随机划分为70%和30%，70%的样本集合作为训练集（样本数量为$m$），30%的样本集合作为测试集($m_{test}$)；\n",
    "\n",
    "** 评估流程为：**\n",
    "\n",
    "1.从训练数据集中学得参数$\\theta$（从最小化训练误差$J(\\theta)$得来）；\n",
    "\n",
    "2.计算测试集上的误差:\n",
    "\n",
    "线性回归：$$J_{test}(\\theta)=\\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$$\n",
    "\n",
    "逻辑回归：$$J_{test}(\\theta)=-\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}(y_{test}^{(i)}log h_{\\theta}(x_{test}^{(i)})+(1-y_{test}^{(i)})log h_{\\theta}(x_{test}^{(i)}))$$\n",
    "\n",
    "逻辑回归的错误分类率（0/1分类率）：\n",
    "定义分类错误函数：$$err(h_{\\theta}(x),y)=\\begin{equation}\n",
    "\\begin{cases} 1 \\quad if \\quad h_{\\theta}(x)\\geq0.5,y=0 \\quad or\\quad h_{\\theta}(x)\\leq0.5,y=1\\\\\n",
    "0 \\quad else\n",
    "\\end{cases}\\end{equation}$$\n",
    "\n",
    "则测试集上的错误分类率为$$test\\quad error=\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)}),y_{test}^{(i)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection 模型选择\n",
    "\n",
    "首先将假设函数假设为样本的不同幂次多项式假设，从这些幂次多项式假设函数中选择合适的模型\n",
    "\n",
    "将原始训练数据集随机划分为训练集（60%，$(x^{1},y^{1}),\\cdots,(x^{m},y^{m})$），交叉验证集（Cross Validation）（20%，$(x_{CV}^{1},y_{CV}^{1}),\\cdots,(x_{CV}^{m_{cv}},y_{CV}^{m_{cv}})$），测试集(20%，$(x_{test}^{1},y_{test}^{1}),\\cdots,(x_{test}^{m_{test}},y_{test}^{m_{test}})$)\n",
    "\n",
    "Training error 训练误差：\n",
    "$$J_{train}(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "Cross Validation error 交叉验证误差：\n",
    "$$J_{cv}(\\theta)=\\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_{\\theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^2$$\n",
    "\n",
    "Test error 测试误差：\n",
    "$$J_{test}(\\theta)=\\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$$\n",
    "\n",
    "**选择具有最小验证误差的幂次多项式假设函数，然后通过测试误差估计模型泛化误差**\n",
    "\n",
    "**Learning Curve(学习曲线)**\n",
    "\n",
    "$J_{cv}(\\theta)$与$J_{train}(\\theta)$随着训练数据集大小增加而变化的曲线图，起始时，$J_{cv}(\\theta)的值将很大，$J_{train}(\\theta)$的值将很小\n",
    "\n",
    "## 诊断偏差（Bias）和方差（Variance）:\n",
    "\n",
    "**High bias(Underfitting) **\n",
    "\n",
    "原因：\n",
    "\n",
    "1.正则化系数$\\lambda$值很大时；\n",
    "\n",
    "2.$J_{train}(\\theta)$值很大并且$J_{cv}(\\theta)$约等于$J_{train}(\\theta)$；\n",
    "\n",
    "3.随着训练数据集大小增加，$J_{cv}(\\theta)$将收敛至等于$J_{train}(\\theta)$，并且此时$J_{cv}(\\theta)$与$J_{train}(\\theta)$的值都相对较大,并且此时学习曲线在训练数据大小增加时，$J_{cv}(\\theta)$与$J_{train}(\\theta)$将趋向平滑即不再变化，此时增加训练数据集的大小将无法再改变学习算法的准确度；\n",
    "\n",
    "**High Variance (Overfitting)**\n",
    "\n",
    "1.当正则化系数$\\lambda$值很小时；\n",
    "\n",
    "2.$J_{train}(\\theta)$值很小并且$J_{cv}(\\theta)$值远大于$J_{train}(\\theta)$；\n",
    "\n",
    "3.随着训练数据集大小增加时，$J_{cv}(\\theta)$与$J_{train}(\\theta)$将分别趋向收敛，但在一定的训练数据集内$J_{cv}(\\theta)$与$J_{train}(\\theta)$之间将存在间隙，但随着训练数据集的增加至无限时，两条曲线将趋向收敛至相等，即增加训练数据集的大小将可以改变学习算法的准确度；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
